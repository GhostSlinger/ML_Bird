

1. Pass in packages
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

2. Create an Agent Script (ex: BirdAgent)

3. Let your agent script inherit from Agent

4. Methods needed:
OnEpisodeBegin()
CollectObservations(VectorSensor sensor)
OnActionReceived(ActionBuffers actionBuffers)

5. Pass in config file 
- update the name of the config file
- update the behaviour name of the config file

6. Set MaxStep = 3000 on your BirdAgent Script

7. Set Behavior Parameters
- Space size: 1 			// this cant be 0 or it will crash! 
- Discrete branch: 1
- Branch 0 size: 5


8. Add in Decision Requester component on Bird


9. Pass in logics OnAction method

10. Add in speed multiplier on the Bird

11. Handle reseting the agent

12. Collect Observations:
- add in component Ray Perception Sensor 2D
- Ray Layer Mask: Coin, set layer to the obstacles you want the agent to see
- Max Ray Degrees: 180
- Rays per Direction: 15


13. Options to play with
- crank up the ray per direction -> 40




15. cd C:\RmitUniMaster\Projects-Unity\Practice\BirdGame_Starter

16. tensorboard --logdir=results



===================================================================================================

99. Note: Check module week 8, there is 
Understanding the ML-Agents .yaml file 
(the file that contains most of the parameters for the learning algorithm):
 https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Configuration-File.md


Config yaml file explanation:


behaviors:
  BirdBehaviour:
    trainer_type: ppo			// proximal policy optimisation
    hyperparameters:
      batch_size: 10			// how many of those 100 its going to sample and learn, select random 10
      buffer_size: 100			// how many observations its gonna collect before attempts to learn
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3				// how many training updates it does per bulk collection of 100 buffer_size
								// collect 100, randomly select 10 to train on, 3 times and done
	  
	  
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000			// Output on graphs after 10000 steps, lower this for better graphs